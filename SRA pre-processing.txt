# Process for downloading and analysing GSE193123 ####


# Downloading files from NCBI
# 1. Create file with accessions needed to download them 
nano srr_ids.txt

# Type out all the SRR numbers in the file
*#!/bin/bash 
SRR_IDS=("SRR3986316" "SRR3986317" "SRR3986318" "SRR3986319" "SRR3986320" "SRR3986321" "SRR3986322" "SRR3986323" "SRR3986326" "SRR3986327" "SRR3986339" "SRR3986340" "SRR3986341" "SRR3986342" "SRR3986343" "SRR3986344" "SRR3986345" "SRR3986346" "SRR3986348" "SRR3986350")



# Copy content of srr_ids.txt into a new file
nano download_sra.sh

# make file executable
chmod +x download_sra.sh
$ #!/bin/bash


# 2. # Specify the output directory for downloaded files
OUTPUT_DIR="./sra_files"

Create the output directory 
mkdir -p "$OUTPUT_DIR"



# 3. If you don't have Aspera, install it as prefetch needs it. Download it from:
https://www.ibm.com/products/aspera/downloads#cds

# 4. List the SRR IDs directly in an array
SRR_IDS=("SRR3986316" "SRR3986317" "SRR3986318" "SRR3986319" "SRR3986320" "SRR3986321" "SRR3986322" "SRR3986323" "SRR3986326" "SRR3986327" "SRR3986339" "SRR3986340" "SRR3986341" "SRR3986342" "SRR3986343" "SRR3986344" "SRR3986345" "SRR3986346" "SRR3986348" "SRR3986350")

# Loop through each SRR ID in the array and download with prefetch
for SRR_ID in "${SRR_IDS[@]}"; do
    echo "Downloading $SRR_ID..."
    prefetch -v --max-size 100G --output-directory "$OUTPUT_DIR" "$SRR_ID"
done

# Check downloaded files 
vdb-validate "$OUTPUT_DIR" 

# 4. Moving .sra files to one directory for easy access
# Define root directory for .sra files
root_dir="/home/biem/thesis/rnasamples/sra_files"

# Define the destination directory
dest_dir="/home/biem/thesis/rnasamples/files_destination"

# Create destinantion directory 
mkdir -p "$dest_dir"

# Find and move all .sra files to the destination directory
find "$root_dir" -type f -name "*.sra" -exec mv {} "$dest_dir" \;

# Get feedback
echo "All.sra files have been moved to dest_dir"


# 5. Convert all the .sra files in the directory to fastq format and compress them
for FILE in /home/biem/thesis/rnasamples/files_destination/*.sra; do
    fastq-dump --split-files --gzip "$FILE"
done


# 6. Run fastqc and multiqc
# Create directory  for fastQC results
mkdir -p fastqc_results

# Run fastQC
fastqc -o fastqc_results *fastq.gz

# Run multiQC
multiqc fastqc_results -o multiqc_results


& 7. Trimming with Trim Galore! (and running QC simultaneously)
# Create directory to store results
mkdir -p trimmed_files

# Create a script
nano trim_galore_script.sh

# Paste
#!/bin/bash
set -e  # Exit if any command fails

mkdir -p trimmed_files

for FILE in *_1.fastq.gz; do
    BASENAME=$(basename "$FILE" _1.fastq.gz)
    echo "Processing $BASENAME"

    if trim_galore --paired --fastqc -q 20 --gzip -o trimmed_files \
        "${BASENAME}_1.fastq.gz" "${BASENAME}_2.fastq.gz"; then
        echo "Successfully trimmed $BASENAME"
    else
        echo "Error: Trim Galore failed for $BASENAME" >&2
        exit 1  # Stop the script on failure
    fi
done

echo "Finished processing all files."

# Make the script executable
chmod +x trim_galore_script.sh

# Execute the script
bash trim_galore_script.sh


# 8. Build the reference genome 
# Create working directory
mkdir -p ref_genome && cd ref_genome  # Fixed directory name

# Download reference genome and annotation files with resumption enabled
wget -c ftp://ftp.ensembl.org/pub/release-109/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz
wget -c ftp://ftp.ensembl.org/pub/release-109/gtf/homo_sapiens/Homo_sapiens.GRCh38.109.gtf.gz

# Decompress files while keeping originals (-k)
gunzip -k Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz
gunzip -k Homo_sapiens.GRCh38.109.gtf.gz

# Build Hisat2 index with multi-threading and logging
hisat2-build -p 8 Homo_sapiens.GRCh38.dna.primary_assembly.fa genome_index 2>&1 | tee hisat2_index.log

# Exit directory
cd ..


# 9. Extract splice sites and align reads to genome
# Step 1: Create output directories
mkdir -p aligned_reads hisat2_splice_sites

# Step 2: Extract known splice sites and exons from the annotation file
hisat2_extract_splice_sites.py genome/Homo_sapiens.GRCh38.109.gtf > hisat2_splice_sites/splice_sites.txt
hisat2_extract_exons.py genome/Homo_sapiens.GRCh38.109.gtf > hisat2_splice_sites/exons.txt

# Step 3: Align reads using HISAT2 with known splice sites and @RG tags    *****IMPORTANT: HISAT2 mapping should include strandness option "fr-firststrand" 
for FILE in trimmed_files/*_val_1.fq.gz; do
  BASENAME=$(basename "$FILE" _1_val_1.fq.gz)  
  echo "Aligning $BASENAME with splice sites..."

  hisat2 -p 4 -x ref_genome/genome_index \
         --known-splicesite-infile /home/biem/thesis/rnasamples/files_destination/ref_genome/hisat2_splice_sites/splice_sites.txt \
         --rg-line "@RG\tID:$BASENAME\tSM:$BASENAME\tLB:lib1\tPL:ILLUMINA\tPU:unit1" \
         -1 "trimmed_files/${BASENAME}_1_val_1.fq.gz" \
         -2 "trimmed_files/${BASENAME}_2_val_2.fq.gz" \
         -S "aligned_reads/${BASENAME}.sam"

done


# 10. Convert from SAM to BAM 
# Create directory for BAM files
mkdir -p sorted_bam

for SAM in aligned_reads/*.sam; do
  BASENAME=$(basename "$SAM" .sam)
  echo "Processing $BASENAME..."

  # Convert, sort, and index in one step
  samtools sort -@ 4 -o "sorted_bam/${BASENAME}_sorted.bam" "$SAM"

  # Index the BAM file
  samtools index "sorted_bam/${BASENAME}_sorted.bam"

  # Remove the original SAM file to save space
  rm "$SAM"
done


# 11. Post-alignment QC
# Create output directories if they don't exist
mkdir -p samtools_stats qualimap_reports

# Loop through all sorted BAM files in the directory
for BAM in sorted_bam/*_sorted.bam; do  
    BASENAME=$(basename "$BAM" _sorted.bam)  

# Generate Samtools alignment statistics and save output
    samtools flagstat "$BAM" > "samtools_stats/${BASENAME}_alignment_flagstat.txt"
    samtools stats "$BAM" > "samtools_stats/${BASENAME}_alignment_stats.txt"

 # Run Qualimap QC for each sample and save reports in a dedicated folder
    qualimap bamqc -bam "$BAM" -outdir "qualimap_reports/${BASENAME}_qualimap" -nt 4
done


# 12. Determine read specificity
qualimap rnaseq -bam sorted_bam/SRR3986322_sorted.bam \
-gtf /home/biem/thesis/rnasamples/files_destination/ref_genome/Homo_sapiens.GRCh38.109.gtf \
-outdir qualimap_reports/SRR3986322_qualimap \
--java-mem-size=4000M
## results indicated reverse strand so I'm going with -s 2 in feature counts

# 13. Generate count matrix for all BAM files in sorted_bam/
featureCounts -a /home/biem/thesis/rnasamples/files_destination/ref_genome/Homo_sapiens.GRCh38.109.gtf \
              -o gene_counts.txt \
              -T 4 \
              -p \
              -t exon \
              -g gene_id \
              -s 2 \
              sorted_bam/*_sorted.bam
####results are poor. Trying with htseq

# 13. Generate count matric for all BAM files in sorted_bam/ [with htseq]
for bam in sorted_bam/*_sorted.bam; do
    sample_name=$(basename "$bam" _sorted.bam)  # Extract sample name
    htseq-count -f bam -s reverse "$bam" /home/biem/thesis/rnasamples/files_destination/ref_genome/Homo_sapiens.GRCh38.109.gtf > "counts_${sample_name}.txt"
done
